diff --git a/configs/config.yaml b/configs/config.yaml
index 05d0f20..f2e90f1 100755
--- a/configs/config.yaml
+++ b/configs/config.yaml
@@ -2,7 +2,8 @@
 task_name: ${task.name}
 
 # if set to positive integer, overrides the default number of environments
-num_envs: ''
+# num_envs: ''
+num_envs: 16 
 
 # seed - set to -1 to choose random seed
 seed: 42
diff --git a/configs/task/OpenManipulator.yaml b/configs/task/OpenManipulator.yaml
deleted file mode 100755
index e2634b1..0000000
--- a/configs/task/OpenManipulator.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
-# used to create the object
-name: OpenManipulatorPick
-physics_engine: ${..physics_engine}
-on_evaluation: False
-
-env:
-  # environment basic
-  numEnvs: ${resolve_default:16384,${...num_envs}}
-  numObservations: 96
-  numActions: 16
-  envSpacing: 0.25
-
-  episodeLength: 400
-  enableDebugVis: False
-  aggregateMode: 1
-
-  controller:
-    torque_control: True
-    controlFrequencyInv: 6  # 20Hz
-    pgain: 3
-    dgain: 0.1
-
-  genGrasps: False
-
-  clipObservations: 5.0
-  clipActions: 1.0
-  reset_height_threshold: 0.645
-  # grasp_cache_name: 'internal_openManipulator'
-
-  # Random forces applied to the object
-  forceScale: 0.0
-  randomForceProbScalar: 0.0
-  forceDecay: 0.9
-  forceDecayInterval: 0.08
-
-  hora:
-    propHistoryLen: 30
-    privInfoDim: 9
-
-  reward:
-    angvelClipMin: -0.5
-    angvelClipMax: 0.5
-    rotateRewardScale: 1.0
-    objLinvelPenaltyScale: -0.3
-    poseDiffPenaltyScale: -0.3
-    torquePenaltyScale: -0.1
-    workPenaltyScale: -2.0
-
-  baseObjScale: 0.8
-  randomization:
-    # Randomization Property
-    randomizeMass: True
-    randomizeMassLower: 0.01
-    randomizeMassUpper: 0.25
-    randomizeCOM: True
-    randomizeCOMLower: -0.01
-    randomizeCOMUpper: 0.01
-    randomizeFriction: True
-    randomizeFrictionLower: 0.3
-    randomizeFrictionUpper: 3.0
-    randomizeScale: True
-    # if scale_list_init is True, randomizeScaleLower and randomizeScaleUpper is not useful
-    scaleListInit: True
-    randomizeScaleList: [0.7, 0.72, 0.74, 0.76, 0.78, 0.8, 0.82, 0.84, 0.86]
-    randomizeScaleLower: 0.75  # only useful when not scaleListInit
-    randomizeScaleUpper: 0.8  # only useful when not scaleListInit
-    randomizePDGains: True
-    randomizePGainLower: 2.9
-    randomizePGainUpper: 3.1
-    randomizeDGainLower: 0.09
-    randomizeDGainUpper: 0.11
-    jointNoiseScale: 0.02
-
-  privInfo:
-    enableObjPos: True
-    enableObjScale: True
-    enableObjMass: True
-    enableObjCOM: True
-    enableObjFriction: True
-
-  object:
-    type: 'block' # can be block, egg or pen
-    sampleProb: [1.0]
-
-  asset:
-    handAsset: 'assets/allegro/allegro_internal.urdf'
-
-  # set to True if you use camera sensors in the environment
-  enableCameraSensors: False
-
-sim:
-  dt: 0.0083333 # 120 Hz
-  substeps: 1
-  up_axis: 'z'
-  use_gpu_pipeline: ${eq:${...pipeline},'gpu'}
-  gravity: [0.0, 0.0, -9.81]
-  physx:
-    num_threads: ${....num_threads}
-    solver_type: ${....solver_type}
-    use_gpu: ${contains:'cuda',${....sim_device}} # set to False to run on CPU
-    num_position_iterations: 8
-    num_velocity_iterations: 0
-    max_gpu_contact_pairs: 8388608 # 8*1024*1024
-    num_subscenes: ${....num_subscenes}
-    contact_offset: 0.002
-    rest_offset: 0.0
-    bounce_threshold_velocity: 0.2
-    max_depenetration_velocity: 1000.0
-    default_buffer_size_multiplier: 5.0
-    contact_collection: 2 # 0: CC_NEVER (don't collect contact info), 1: CC_LAST_SUBSTEP (collect only contacts on last substep), 2: CC_ALL_SUBSTEPS (default - all contacts)
diff --git a/configs/train/OpenManipulator.yaml b/configs/train/OpenManipulator.yaml
deleted file mode 100755
index 2d146f5..0000000
--- a/configs/train/OpenManipulator.yaml
+++ /dev/null
@@ -1,43 +0,0 @@
-seed: ${..seed}
-algo: PPO
-network:
-  mlp:
-    units: [512, 256, 128]
-  priv_mlp:
-    units: [256, 128, 8]
-
-load_path: ${..checkpoint} # path to the checkpoint to load
-
-ppo:
-  output_name: 'debug'
-  normalize_input: True
-  normalize_value: True
-  value_bootstrap: True
-  num_actors: ${...task.env.numEnvs}
-  normalize_advantage: True
-  gamma: 0.99
-  tau: 0.95
-  learning_rate: 5e-3
-  kl_threshold: 0.02
-  # PPO batch collection
-  horizon_length: 8
-  minibatch_size: 32768
-  mini_epochs: 5
-  # PPO loss setting
-  clip_value: True
-  critic_coef: 4
-  entropy_coef: 0.0
-  e_clip: 0.2
-  bounds_loss_coef: 0.0001
-  # grad clipping
-  truncate_grads: True
-  grad_norm: 1.0
-  # snapshot setting
-  save_best_after: 0
-  save_frequency: 500
-  max_agent_steps: 1500000000
-  # hora setting
-  priv_info: False
-  priv_info_dim: 9
-  priv_info_embed_dim: 8
-  proprio_adapt: False
diff --git a/manipulation_gym/algo/padapt/padapt.py b/manipulation_gym/algo/padapt/padapt.py
index 29d4c8d..c5ab855 100755
--- a/manipulation_gym/algo/padapt/padapt.py
+++ b/manipulation_gym/algo/padapt/padapt.py
@@ -11,9 +11,9 @@ import torch
 import numpy as np
 from termcolor import cprint
 
-from hora.utils.misc import AverageScalarMeter, tprint
-from hora.algo.models.models import ActorCritic
-from hora.algo.models.running_mean_std import RunningMeanStd
+from manipulation_gym.utils.misc import AverageScalarMeter, tprint
+from manipulation_gym.algo.models.models import ActorCritic
+from manipulation_gym.algo.models.running_mean_std import RunningMeanStd
 from tensorboardX import SummaryWriter
 
 
diff --git a/manipulation_gym/algo/ppo/__pycache__/__init__.cpython-38.pyc b/manipulation_gym/algo/ppo/__pycache__/__init__.cpython-38.pyc
old mode 100755
new mode 100644
index 64d5a7b..1158449
Binary files a/manipulation_gym/algo/ppo/__pycache__/__init__.cpython-38.pyc and b/manipulation_gym/algo/ppo/__pycache__/__init__.cpython-38.pyc differ
diff --git a/manipulation_gym/algo/ppo/__pycache__/experience.cpython-38.pyc b/manipulation_gym/algo/ppo/__pycache__/experience.cpython-38.pyc
old mode 100755
new mode 100644
index f7d7492..827c024
Binary files a/manipulation_gym/algo/ppo/__pycache__/experience.cpython-38.pyc and b/manipulation_gym/algo/ppo/__pycache__/experience.cpython-38.pyc differ
diff --git a/manipulation_gym/algo/ppo/__pycache__/ppo.cpython-38.pyc b/manipulation_gym/algo/ppo/__pycache__/ppo.cpython-38.pyc
old mode 100755
new mode 100644
index 613ec38..9a6ddb6
Binary files a/manipulation_gym/algo/ppo/__pycache__/ppo.cpython-38.pyc and b/manipulation_gym/algo/ppo/__pycache__/ppo.cpython-38.pyc differ
diff --git a/manipulation_gym/algo/ppo/ppo.py b/manipulation_gym/algo/ppo/ppo.py
index 874d220..c298210 100755
--- a/manipulation_gym/algo/ppo/ppo.py
+++ b/manipulation_gym/algo/ppo/ppo.py
@@ -14,11 +14,11 @@ import os
 import time
 import torch
 
-from hora.algo.ppo.experience import ExperienceBuffer
-from hora.algo.models.models import ActorCritic
-from hora.algo.models.running_mean_std import RunningMeanStd
+from manipulation_gym.algo.ppo.experience import ExperienceBuffer
+from manipulation_gym.algo.models.models import ActorCritic
+from manipulation_gym.algo.models.running_mean_std import RunningMeanStd
 
-from hora.utils.misc import AverageScalarMeter
+from manipulation_gym.utils.misc import AverageScalarMeter
 
 from tensorboardX import SummaryWriter
 
@@ -28,6 +28,9 @@ class PPO(object):
         self.device = full_config['rl_device']
         self.network_config = full_config.train.network
         self.ppo_config = full_config.train.ppo
+
+        
+
         # ---- build environment ----
         self.env = env
         self.num_actors = self.ppo_config['num_actors']
@@ -55,6 +58,7 @@ class PPO(object):
         self.model.to(self.device)
         self.running_mean_std = RunningMeanStd(self.obs_shape).to(self.device)
         self.value_mean_std = RunningMeanStd((1,)).to(self.device)
+
         # ---- Output Dir ----
         # allows us to specify a folder where all experiments will reside
         self.output_dir = output_dif
@@ -80,12 +84,16 @@ class PPO(object):
         self.normalize_advantage = self.ppo_config['normalize_advantage']
         self.normalize_input = self.ppo_config['normalize_input']
         self.normalize_value = self.ppo_config['normalize_value']
+
         # ---- PPO Collect Param ----
         self.horizon_length = self.ppo_config['horizon_length']
         self.batch_size = self.horizon_length * self.num_actors
         self.minibatch_size = self.ppo_config['minibatch_size']
         self.mini_epochs_num = self.ppo_config['mini_epochs']
         assert self.batch_size % self.minibatch_size == 0 or full_config.test
+
+        
+
         # ---- scheduler ----
         self.kl_threshold = self.ppo_config['kl_threshold']
         self.scheduler = AdaptiveScheduler(self.kl_threshold)
@@ -119,6 +127,8 @@ class PPO(object):
         self.rl_train_time = 0
         self.all_time = 0
 
+        print("PPO INIT COMPLETE")
+
     def write_stats(self, a_losses, c_losses, b_losses, entropies, kls):
         self.writer.add_scalar('performance/RLTrainFPS', self.agent_steps / self.rl_train_time, self.agent_steps)
         self.writer.add_scalar('performance/EnvStepFPS', self.agent_steps / self.data_collect_time, self.agent_steps)
diff --git a/manipulation_gym/tasks/__init__.py b/manipulation_gym/tasks/__init__.py
index 319028b..30809e1 100755
--- a/manipulation_gym/tasks/__init__.py
+++ b/manipulation_gym/tasks/__init__.py
@@ -27,13 +27,11 @@
 # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 
-from hora.tasks.allegro_hand_hora import AllegroHandHora
-from hora.tasks.allegro_hand_grasp import AllegroHandGrasp
+from manipulation_gym.tasks.open_manipulator_pick import OpenManipulatorPick
+from manipulation_gym.tasks.open_manipulator_place import OpenManipulatorPlace
 
 # Mappings from strings to environments
 isaacgym_task_map = {
-    'AllegroHandHora': AllegroHandHora,
-    'AllegroHandGrasp': AllegroHandGrasp,
-    'PublicAllegroHandHora': AllegroHandHora,
-    'PublicAllegroHandGrasp': AllegroHandGrasp,
+    'OpenManipulatorPick': OpenManipulatorPick,
+    'OpenManipulatorPlace': OpenManipulatorPlace,
 }
\ No newline at end of file
diff --git a/manipulation_gym/tasks/allegro_hand_grasp.py b/manipulation_gym/tasks/allegro_hand_grasp.py
deleted file mode 100755
index 1c04259..0000000
--- a/manipulation_gym/tasks/allegro_hand_grasp.py
+++ /dev/null
@@ -1,140 +0,0 @@
-# --------------------------------------------------------
-# In-Hand Object Rotation via Rapid Motor Adaptation
-# https://arxiv.org/abs/2210.04887
-# Copyright (c) 2022 Haozhi Qi
-# Licensed under The MIT License [see LICENSE for details]
-# --------------------------------------------------------
-
-import torch
-import numpy as np
-from isaacgym import gymtorch
-from isaacgym.torch_utils import torch_rand_float, quat_from_angle_axis, quat_mul, tensor_clamp, to_torch
-from hora.tasks.allegro_hand_hora import AllegroHandHora
-
-
-class AllegroHandGrasp(AllegroHandHora):
-    def __init__(self, config, sim_device, graphics_device_id, headless):
-        super().__init__(config, sim_device=sim_device, graphics_device_id=graphics_device_id, headless=headless)
-        self.saved_grasping_states = torch.zeros((0, 23), dtype=torch.float, device=self.device)
-        self.canonical_pose = [
-            0.082, 1.244, 0.265, 0.298, 1.104, 1.163, 0.953, -0.138,
-            0.005, 1.096, 0.080, 0.150, 0.029, 1.337, 0.285, 0.317,
-        ]
-        self.x_unit_tensor = to_torch([1, 0, 0], dtype=torch.float, device=self.device).repeat((self.num_envs, 1))
-        self.y_unit_tensor = to_torch([0, 1, 0], dtype=torch.float, device=self.device).repeat((self.num_envs, 1))
-        self.z_unit_tensor = to_torch([0, 0, 1], dtype=torch.float, device=self.device).repeat((self.num_envs, 1))
-
-    def reset_idx(self, env_ids):
-        if self.randomize_mass:
-            lower, upper = self.randomize_mass_lower, self.randomize_mass_upper
-            for env_id in env_ids:
-                env = self.envs[env_id]
-                handle = self.gym.find_actor_handle(env, 'object')
-                prop = self.gym.get_actor_rigid_body_properties(env, handle)
-                for p in prop:
-                    p.mass = np.random.uniform(lower, upper)
-                self.gym.set_actor_rigid_body_properties(env, handle, prop)
-                self._update_priv_buf(env_id=env_id, name='obj_mass', value=prop[0].mass, lower=0, upper=0.2)
-        else:
-            for env_id in env_ids:
-                env = self.envs[env_id]
-                handle = self.gym.find_actor_handle(env, 'object')
-                prop = self.gym.get_actor_rigid_body_properties(env, handle)
-                self._update_priv_buf(env_id=env_id, name='obj_mass', value=prop[0].mass, lower=0, upper=0.2)
-
-        if self.randomize_pd_gains:
-            self.p_gain[env_ids] = torch_rand_float(
-                self.randomize_p_gain_lower, self.randomize_p_gain_upper, (len(env_ids), self.num_actions),
-                device=self.device).squeeze(1)
-            self.d_gain[env_ids] = torch_rand_float(
-                self.randomize_d_gain_lower, self.randomize_d_gain_upper, (len(env_ids), self.num_actions),
-                device=self.device).squeeze(1)
-
-        # generate random values
-        rand_floats = torch_rand_float(-1.0, 1.0, (len(env_ids), self.num_allegro_hand_dofs * 2 + 5), device=self.device)
-
-        # reset rigid body forces
-        self.rb_forces[env_ids, :, :] = 0.0
-        success = self.progress_buf[env_ids] == self.max_episode_length
-        all_states = torch.cat([
-            self.allegro_hand_dof_pos, self.root_state_tensor[self.object_indices, :7]
-        ], dim=1)
-        self.saved_grasping_states = torch.cat([self.saved_grasping_states, all_states[env_ids][success]])
-        print('current cache size:', self.saved_grasping_states.shape[0])
-        if len(self.saved_grasping_states) >= 5e4:
-            name = f'cache/{self.grasp_cache_name}_grasp_50k_s{str(self.base_obj_scale).replace(".", "")}.npy'
-            np.save(name, self.saved_grasping_states[:50000].cpu().numpy())
-            exit()
-
-        # reset object
-        self.root_state_tensor[self.object_indices[env_ids]] = self.object_init_state[env_ids].clone()
-        self.root_state_tensor[self.object_indices[env_ids], 0:2] = self.object_init_state[env_ids, 0:2]
-        self.root_state_tensor[self.object_indices[env_ids], self.up_axis_idx] = self.object_init_state[env_ids, self.up_axis_idx]
-        new_object_rot = randomize_rotation(rand_floats[:, 3], rand_floats[:, 4], self.x_unit_tensor[env_ids], self.y_unit_tensor[env_ids])
-        new_object_rot[:] = 0
-        new_object_rot[:, -1] = 1
-        self.root_state_tensor[self.object_indices[env_ids], 3:7] = new_object_rot
-        self.root_state_tensor[self.object_indices[env_ids], 7:13] = torch.zeros_like(
-            self.root_state_tensor[self.object_indices[env_ids], 7:13])
-
-        object_indices = torch.unique(self.object_indices[env_ids]).to(torch.int32)
-        self.gym.set_actor_root_state_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.root_state_tensor),
-                                                     gymtorch.unwrap_tensor(object_indices), len(object_indices))
-
-        pos = to_torch(self.canonical_pose, device=self.device)[None].repeat(len(env_ids), 1)
-        pos += 0.25 * rand_floats[:, 5:5 + self.num_allegro_hand_dofs]
-        pos = tensor_clamp(pos, self.allegro_hand_dof_lower_limits, self.allegro_hand_dof_upper_limits)
-
-        self.allegro_hand_dof_pos[env_ids, :] = pos
-        self.allegro_hand_dof_vel[env_ids, :] = 0
-        self.prev_targets[env_ids, :self.num_allegro_hand_dofs] = pos
-        self.cur_targets[env_ids, :self.num_allegro_hand_dofs] = pos
-
-        hand_indices = self.hand_indices[env_ids].to(torch.int32)
-        if not self.torque_control:
-            self.gym.set_dof_position_target_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.prev_targets),
-                                                            gymtorch.unwrap_tensor(hand_indices), len(env_ids))
-        self.gym.set_dof_state_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.dof_state),
-                                              gymtorch.unwrap_tensor(hand_indices), len(env_ids))
-
-        self.progress_buf[env_ids] = 0
-        self.obs_buf[env_ids] = 0
-        self.rb_forces[env_ids] = 0
-        self.priv_info_buf[env_ids, 0:3] = 0
-        self.proprio_hist_buf[env_ids] = 0
-
-        self.at_reset_buf[env_ids] = 1
-
-    def compute_reward(self, actions):
-        def list_intersect(li, hash_num):
-            # 17 is the object index
-            # 4, 8, 12, 16 are fingertip index
-            # return number of contact with obj_id
-            obj_id = 17
-            query_list = [obj_id * hash_num + 4, obj_id * hash_num + 8, obj_id * hash_num + 12, obj_id * hash_num + 16]
-            return len(np.intersect1d(query_list, li))
-        assert self.device == 'cpu'
-        contacts = [self.gym.get_env_rigid_contacts(env) for env in self.envs]
-        contact_list = [list_intersect(np.unique([c[2] * 10000 + c[3] for c in contact]), 10000) for contact in contacts]
-        contact_condition = to_torch(contact_list, device=self.device)
-
-        obj_pos = self.rigid_body_states[:, [-1], :3]
-        finger_pos = self.rigid_body_states[:, [4, 8, 12, 16], :3]
-        # the sampled pose need to satisfy (check 1 here):
-        # 1) all fingertips is nearby objects
-        cond1 = (torch.sqrt(((obj_pos - finger_pos) ** 2).sum(-1)) < 0.1).all(-1)
-        # 2) at least two fingers are in contact with object
-        cond2 = contact_condition >= 2
-        # 3) object does not fall after a few iterations
-        # 0.645 for internal allegro
-        # 0.625 for public allegro
-        cond3 = torch.greater(obj_pos[:, -1, -1], self.reset_z_threshold)
-        cond = cond1.float() * cond2.float() * cond3.float()
-        # reset if any of the above condition does not hold
-        self.reset_buf[cond < 1] = 1
-        self.reset_buf[self.progress_buf >= self.max_episode_length] = 1
-
-
-@torch.jit.script
-def randomize_rotation(rand0, rand1, x_unit_tensor, y_unit_tensor):
-    return quat_mul(quat_from_angle_axis(rand0 * np.pi, x_unit_tensor), quat_from_angle_axis(rand1 * np.pi, y_unit_tensor))
diff --git a/manipulation_gym/tasks/allegro_hand_hora.py b/manipulation_gym/tasks/allegro_hand_hora.py
deleted file mode 100755
index c1acf76..0000000
--- a/manipulation_gym/tasks/allegro_hand_hora.py
+++ /dev/null
@@ -1,640 +0,0 @@
-# --------------------------------------------------------
-# In-Hand Object Rotation via Rapid Motor Adaptation
-# https://arxiv.org/abs/2210.04887
-# Copyright (c) 2022 Haozhi Qi
-# Licensed under The MIT License [see LICENSE for details]
-# --------------------------------------------------------
-
-import os
-import torch
-import numpy as np
-from isaacgym import gymtorch
-from isaacgym import gymapi
-from isaacgym.torch_utils import to_torch, unscale, quat_apply, tensor_clamp, torch_rand_float
-from glob import glob
-from hora.utils.misc import tprint
-from .base.vec_task import VecTask
-
-
-class AllegroHandHora(VecTask):
-    def __init__(self, config, sim_device, graphics_device_id, headless):
-        self.config = config
-        # before calling init in VecTask, need to do
-        # 1. setup randomization
-        self._setup_domain_rand_config(config['env']['randomization'])
-        # 2. setup privileged information
-        self._setup_priv_option_config(config['env']['privInfo'])
-        # 3. setup object assets
-        self._setup_object_info(config['env']['object'])
-        # 4. setup reward
-        self._setup_reward_config(config['env']['reward'])
-        self.base_obj_scale = config['env']['baseObjScale']
-        self.save_init_pose = config['env']['genGrasps']
-        self.aggregate_mode = self.config['env']['aggregateMode']
-        self.up_axis = 'z'
-        self.reset_z_threshold = self.config['env']['reset_height_threshold']
-        self.grasp_cache_name = self.config['env']['grasp_cache_name']
-        self.evaluate = self.config['on_evaluation']
-        self.priv_info_dict = {
-            'obj_position': (0, 3),
-            'obj_scale': (3, 4),
-            'obj_mass': (4, 5),
-            'obj_friction': (5, 6),
-            'obj_com': (6, 9),
-        }
-
-        super().__init__(config, sim_device, graphics_device_id, headless)
-
-        self.debug_viz = self.config['env']['enableDebugVis']
-        self.max_episode_length = self.config['env']['episodeLength']
-        self.dt = self.sim_params.dt
-
-        if self.viewer:
-            cam_pos = gymapi.Vec3(0.0, 0.4, 1.5)
-            cam_target = gymapi.Vec3(0.0, 0.0, 0.5)
-            self.gym.viewer_camera_look_at(self.viewer, None, cam_pos, cam_target)
-
-        # get gym GPU state tensors
-        actor_root_state_tensor = self.gym.acquire_actor_root_state_tensor(self.sim)
-        dof_state_tensor = self.gym.acquire_dof_state_tensor(self.sim)
-        rigid_body_tensor = self.gym.acquire_rigid_body_state_tensor(self.sim)
-        net_contact_forces = self.gym.acquire_net_contact_force_tensor(self.sim)
-
-        # create some wrapper tensors for different slices
-        self.allegro_hand_default_dof_pos = torch.zeros(self.num_allegro_hand_dofs, dtype=torch.float, device=self.device)
-        self.dof_state = gymtorch.wrap_tensor(dof_state_tensor)
-        self.contact_forces = gymtorch.wrap_tensor(net_contact_forces).view(self.num_envs, -1, 3)
-        self.allegro_hand_dof_state = self.dof_state.view(self.num_envs, -1, 2)[:, :self.num_allegro_hand_dofs]
-        self.allegro_hand_dof_pos = self.allegro_hand_dof_state[..., 0]
-        self.allegro_hand_dof_vel = self.allegro_hand_dof_state[..., 1]
-
-        self.rigid_body_states = gymtorch.wrap_tensor(rigid_body_tensor).view(self.num_envs, -1, 13)
-        self.num_bodies = self.rigid_body_states.shape[1]
-
-        self.root_state_tensor = gymtorch.wrap_tensor(actor_root_state_tensor).view(-1, 13)
-
-        self._refresh_gym()
-
-        self.num_dofs = self.gym.get_sim_dof_count(self.sim) // self.num_envs
-
-        self.prev_targets = torch.zeros((self.num_envs, self.num_dofs), dtype=torch.float, device=self.device)
-        self.cur_targets = torch.zeros((self.num_envs, self.num_dofs), dtype=torch.float, device=self.device)
-        # object apply random forces parameters
-        self.force_scale = self.config['env'].get('forceScale', 0.0)
-        self.random_force_prob_scalar = self.config['env'].get('randomForceProbScalar', 0.0)
-        self.force_decay = self.config['env'].get('forceDecay', 0.99)
-        self.force_decay_interval = self.config['env'].get('forceDecayInterval', 0.08)
-        self.force_decay = to_torch(self.force_decay, dtype=torch.float, device=self.device)
-        self.rb_forces = torch.zeros((self.num_envs, self.num_bodies, 3), dtype=torch.float, device=self.device)
-
-        if self.randomize_scale and self.scale_list_init:
-            self.saved_grasping_states = {}
-            for s in self.randomize_scale_list:
-                self.saved_grasping_states[str(s)] = torch.from_numpy(np.load(
-                    f'cache/{self.grasp_cache_name}_grasp_50k_s{str(s).replace(".", "")}.npy'
-                )).float().to(self.device)
-        else:
-            assert self.save_init_pose
-
-        self.rot_axis_buf = torch.zeros((self.num_envs, 3), device=self.device, dtype=torch.float)
-
-        # useful buffers
-        self.init_pose_buf = torch.zeros((self.num_envs, self.num_dofs), device=self.device, dtype=torch.float)
-        self.actions = torch.zeros((self.num_envs, self.num_actions), device=self.device, dtype=torch.float)
-        self.torques = torch.zeros((self.num_envs, self.num_actions), device=self.device, dtype=torch.float)
-        self.dof_vel_finite_diff = torch.zeros((self.num_envs, self.num_dofs), device=self.device, dtype=torch.float)
-        assert type(self.p_gain) in [int, float] and type(self.d_gain) in [int, float], 'assume p_gain and d_gain are only scalars'
-        self.p_gain = torch.ones((self.num_envs, self.num_actions), device=self.device, dtype=torch.float) * self.p_gain
-        self.d_gain = torch.ones((self.num_envs, self.num_actions), device=self.device, dtype=torch.float) * self.d_gain
-
-        # debug and understanding statistics
-        self.env_timeout_counter = to_torch(np.zeros((len(self.envs)))).long().to(self.device)  # max 10 (10000 envs)
-        self.stat_sum_rewards = 0
-        self.stat_sum_rotate_rewards = 0
-        self.stat_sum_episode_length = 0
-        self.stat_sum_obj_linvel = 0
-        self.stat_sum_torques = 0
-        self.env_evaluated = 0
-        self.max_evaluate_envs = 500000
-
-    def _create_envs(self, num_envs, spacing, num_per_row):
-        self._create_ground_plane()
-        lower = gymapi.Vec3(-spacing, -spacing, 0.0)
-        upper = gymapi.Vec3(spacing, spacing, spacing)
-
-        self._create_object_asset()
-
-        # set allegro_hand dof properties
-        self.num_allegro_hand_dofs = self.gym.get_asset_dof_count(self.hand_asset)
-        allegro_hand_dof_props = self.gym.get_asset_dof_properties(self.hand_asset)
-
-        self.allegro_hand_dof_lower_limits = []
-        self.allegro_hand_dof_upper_limits = []
-
-        for i in range(self.num_allegro_hand_dofs):
-            self.allegro_hand_dof_lower_limits.append(allegro_hand_dof_props['lower'][i])
-            self.allegro_hand_dof_upper_limits.append(allegro_hand_dof_props['upper'][i])
-            allegro_hand_dof_props['effort'][i] = 0.5
-            if self.torque_control:
-                allegro_hand_dof_props['stiffness'][i] = 0.
-                allegro_hand_dof_props['damping'][i] = 0.
-                allegro_hand_dof_props['driveMode'][i] = gymapi.DOF_MODE_EFFORT
-            else:
-                allegro_hand_dof_props['stiffness'][i] = self.config['env']['controller']['pgain']
-                allegro_hand_dof_props['damping'][i] = self.config['env']['controller']['dgain']
-            allegro_hand_dof_props['friction'][i] = 0.01
-            allegro_hand_dof_props['armature'][i] = 0.001
-
-        self.allegro_hand_dof_lower_limits = to_torch(self.allegro_hand_dof_lower_limits, device=self.device)
-        self.allegro_hand_dof_upper_limits = to_torch(self.allegro_hand_dof_upper_limits, device=self.device)
-
-        hand_pose, obj_pose = self._init_object_pose()
-
-        # compute aggregate size
-        self.num_allegro_hand_bodies = self.gym.get_asset_rigid_body_count(self.hand_asset)
-        self.num_allegro_hand_shapes = self.gym.get_asset_rigid_shape_count(self.hand_asset)
-        max_agg_bodies = self.num_allegro_hand_bodies + 2
-        max_agg_shapes = self.num_allegro_hand_shapes + 2
-
-        self.envs = []
-
-        self.object_init_state = []
-
-        self.hand_indices = []
-        self.object_indices = []
-
-        allegro_hand_rb_count = self.gym.get_asset_rigid_body_count(self.hand_asset)
-        object_rb_count = 1
-        self.object_rb_handles = list(range(allegro_hand_rb_count, allegro_hand_rb_count + object_rb_count))
-
-        for i in range(num_envs):
-            # create env instance
-            env_ptr = self.gym.create_env(self.sim, lower, upper, num_per_row)
-            if self.aggregate_mode >= 1:
-                self.gym.begin_aggregate(env_ptr, max_agg_bodies * 20, max_agg_shapes * 20, True)
-
-            # add hand - collision filter = -1 to use asset collision filters set in mjcf loader
-            hand_actor = self.gym.create_actor(env_ptr, self.hand_asset, hand_pose, 'hand', i, -1, 0)
-            self.gym.set_actor_dof_properties(env_ptr, hand_actor, allegro_hand_dof_props)
-            hand_idx = self.gym.get_actor_index(env_ptr, hand_actor, gymapi.DOMAIN_SIM)
-            self.hand_indices.append(hand_idx)
-
-            # add object
-            object_type_id = np.random.choice(len(self.object_type_list), p=self.object_type_prob)
-            object_asset = self.object_asset_list[object_type_id]
-
-            object_handle = self.gym.create_actor(env_ptr, object_asset, obj_pose, 'object', i, 0, 0)
-            self.object_init_state.append([
-                obj_pose.p.x, obj_pose.p.y, obj_pose.p.z,
-                obj_pose.r.x, obj_pose.r.y, obj_pose.r.z, obj_pose.r.w,
-                0, 0, 0, 0, 0, 0
-            ])
-            object_idx = self.gym.get_actor_index(env_ptr, object_handle, gymapi.DOMAIN_SIM)
-            self.object_indices.append(object_idx)
-
-            obj_scale = self.base_obj_scale
-            if self.randomize_scale:
-                num_scales = len(self.randomize_scale_list)
-                obj_scale = np.random.uniform(self.randomize_scale_list[i % num_scales] - 0.025, self.randomize_scale_list[i % num_scales] + 0.025)
-            self.gym.set_actor_scale(env_ptr, object_handle, obj_scale)
-            self._update_priv_buf(env_id=i, name='obj_scale', value=obj_scale, lower=0.6, upper=0.9)
-
-            obj_com = [0, 0, 0]
-            if self.randomize_com:
-                prop = self.gym.get_actor_rigid_body_properties(env_ptr, object_handle)
-                assert len(prop) == 1
-                obj_com = [np.random.uniform(self.randomize_com_lower, self.randomize_com_upper),
-                           np.random.uniform(self.randomize_com_lower, self.randomize_com_upper),
-                           np.random.uniform(self.randomize_com_lower, self.randomize_com_upper)]
-                prop[0].com.x, prop[0].com.y, prop[0].com.z = obj_com
-                self.gym.set_actor_rigid_body_properties(env_ptr, object_handle, prop)
-            self._update_priv_buf(env_id=i, name='obj_com', value=obj_com, lower=-0.02, upper=0.02)
-
-            obj_friction = 1.0
-            if self.randomize_friction:
-                rand_friction = np.random.uniform(self.randomize_friction_lower, self.randomize_friction_upper)
-                hand_props = self.gym.get_actor_rigid_shape_properties(env_ptr, hand_actor)
-                for p in hand_props:
-                    p.friction = rand_friction
-                self.gym.set_actor_rigid_shape_properties(env_ptr, hand_actor, hand_props)
-
-                object_props = self.gym.get_actor_rigid_shape_properties(env_ptr, object_handle)
-                for p in object_props:
-                    p.friction = rand_friction
-                self.gym.set_actor_rigid_shape_properties(env_ptr, object_handle, object_props)
-                obj_friction = rand_friction
-            self._update_priv_buf(env_id=i, name='obj_friction', value=obj_friction, lower=0.0, upper=1.5)
-
-            if self.aggregate_mode > 0:
-                self.gym.end_aggregate(env_ptr)
-
-            self.envs.append(env_ptr)
-
-        self.object_init_state = to_torch(self.object_init_state, device=self.device, dtype=torch.float).view(self.num_envs, 13)
-        self.object_rb_handles = to_torch(self.object_rb_handles, dtype=torch.long, device=self.device)
-        self.hand_indices = to_torch(self.hand_indices, dtype=torch.long, device=self.device)
-        self.object_indices = to_torch(self.object_indices, dtype=torch.long, device=self.device)
-
-    def reset_idx(self, env_ids):
-        if self.randomize_mass:
-            lower, upper = self.randomize_mass_lower, self.randomize_mass_upper
-
-            for env_id in env_ids:
-                env = self.envs[env_id]
-                handle = self.gym.find_actor_handle(env, 'object')
-                prop = self.gym.get_actor_rigid_body_properties(env, handle)
-                for p in prop:
-                    p.mass = np.random.uniform(lower, upper)
-                self.gym.set_actor_rigid_body_properties(env, handle, prop)
-                self._update_priv_buf(env_id=env_id, name='obj_mass', value=prop[0].mass, lower=0, upper=0.2)
-        else:
-            for env_id in env_ids:
-                env = self.envs[env_id]
-                handle = self.gym.find_actor_handle(env, 'object')
-                prop = self.gym.get_actor_rigid_body_properties(env, handle)
-                self._update_priv_buf(env_id=env_id, name='obj_mass', value=prop[0].mass, lower=0, upper=0.2)
-
-        if self.randomize_pd_gains:
-            self.p_gain[env_ids] = torch_rand_float(
-                self.randomize_p_gain_lower, self.randomize_p_gain_upper, (len(env_ids), self.num_actions),
-                device=self.device).squeeze(1)
-            self.d_gain[env_ids] = torch_rand_float(
-                self.randomize_d_gain_lower, self.randomize_d_gain_upper, (len(env_ids), self.num_actions),
-                device=self.device).squeeze(1)
-
-        # reset rigid body forces
-        self.rb_forces[env_ids, :, :] = 0.0
-
-        num_scales = len(self.randomize_scale_list)
-        for n_s in range(num_scales):
-            s_ids = env_ids[(env_ids % num_scales == n_s).nonzero(as_tuple=False).squeeze(-1)]
-            if len(s_ids) == 0:
-                continue
-            obj_scale = self.randomize_scale_list[n_s]
-            scale_key = str(obj_scale)
-            sampled_pose_idx = np.random.randint(self.saved_grasping_states[scale_key].shape[0], size=len(s_ids))
-            sampled_pose = self.saved_grasping_states[scale_key][sampled_pose_idx].clone()
-            self.root_state_tensor[self.object_indices[s_ids], :7] = sampled_pose[:, 16:]
-            self.root_state_tensor[self.object_indices[s_ids], 7:13] = 0
-            pos = sampled_pose[:, :16]
-            self.allegro_hand_dof_pos[s_ids, :] = pos
-            self.allegro_hand_dof_vel[s_ids, :] = 0
-            self.prev_targets[s_ids, :self.num_allegro_hand_dofs] = pos
-            self.cur_targets[s_ids, :self.num_allegro_hand_dofs] = pos
-            self.init_pose_buf[s_ids, :] = pos.clone()
-
-        object_indices = torch.unique(self.object_indices[env_ids]).to(torch.int32)
-        self.gym.set_actor_root_state_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.root_state_tensor), gymtorch.unwrap_tensor(object_indices), len(object_indices))
-        hand_indices = self.hand_indices[env_ids].to(torch.int32)
-        if not self.torque_control:
-            self.gym.set_dof_position_target_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.prev_targets), gymtorch.unwrap_tensor(hand_indices), len(env_ids))
-        self.gym.set_dof_state_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.dof_state), gymtorch.unwrap_tensor(hand_indices), len(env_ids))
-
-        self.progress_buf[env_ids] = 0
-        self.obs_buf[env_ids] = 0
-        self.rb_forces[env_ids] = 0
-        self.priv_info_buf[env_ids, 0:3] = 0
-        self.proprio_hist_buf[env_ids] = 0
-        self.at_reset_buf[env_ids] = 1
-
-    def compute_observations(self):
-        self._refresh_gym()
-        # deal with normal observation, do sliding window
-        prev_obs_buf = self.obs_buf_lag_history[:, 1:].clone()
-        joint_noise_matrix = (torch.rand(self.allegro_hand_dof_pos.shape) * 2.0 - 1.0) * self.joint_noise_scale
-        cur_obs_buf = unscale(
-            joint_noise_matrix.to(self.device) + self.allegro_hand_dof_pos, self.allegro_hand_dof_lower_limits, self.allegro_hand_dof_upper_limits
-        ).clone().unsqueeze(1)
-        cur_tar_buf = self.cur_targets[:, None]
-        cur_obs_buf = torch.cat([cur_obs_buf, cur_tar_buf], dim=-1)
-        self.obs_buf_lag_history[:] = torch.cat([prev_obs_buf, cur_obs_buf], dim=1)
-
-        # refill the initialized buffers
-        at_reset_env_ids = self.at_reset_buf.nonzero(as_tuple=False).squeeze(-1)
-        self.obs_buf_lag_history[at_reset_env_ids, :, 0:16] = unscale(
-            self.allegro_hand_dof_pos[at_reset_env_ids], self.allegro_hand_dof_lower_limits,
-            self.allegro_hand_dof_upper_limits
-        ).clone().unsqueeze(1)
-        self.obs_buf_lag_history[at_reset_env_ids, :, 16:32] = self.allegro_hand_dof_pos[at_reset_env_ids].unsqueeze(1)
-        t_buf = (self.obs_buf_lag_history[:, -3:].reshape(self.num_envs, -1)).clone()
-
-        self.obs_buf[:, :t_buf.shape[1]] = t_buf
-        self.at_reset_buf[at_reset_env_ids] = 0
-
-        self.proprio_hist_buf[:] = self.obs_buf_lag_history[:, -self.prop_hist_len:].clone()
-        self._update_priv_buf(env_id=range(self.num_envs), name='obj_position', value=self.object_pos.clone())
-
-    def compute_reward(self, actions):
-        self.rot_axis_buf[:, -1] = -1
-        # pose diff penalty
-        pose_diff_penalty = ((self.allegro_hand_dof_pos - self.init_pose_buf) ** 2).sum(-1)
-        # work and torque penalty
-        torque_penalty = (self.torques ** 2).sum(-1)
-        work_penalty = ((self.torques * self.dof_vel_finite_diff).sum(-1)) ** 2
-        obj_linv_pscale = self.object_linvel_penalty_scale
-        pose_diff_pscale = self.pose_diff_penalty_scale
-        torque_pscale = self.torque_penalty_scale
-        work_pscale = self.work_penalty_scale
-
-        self.rew_buf[:], log_r_reward, olv_penalty = compute_hand_reward(
-            self.object_linvel, obj_linv_pscale,
-            self.object_angvel, self.rot_axis_buf, self.rotate_reward_scale,
-            self.angvel_clip_max, self.angvel_clip_min,
-            pose_diff_penalty, pose_diff_pscale,
-            torque_penalty, torque_pscale,
-            work_penalty, work_pscale,
-        )
-        self.reset_buf[:] = self.check_termination(self.object_pos)
-        self.extras['rotation_reward'] = log_r_reward.mean()
-        self.extras['object_linvel_penalty'] = olv_penalty.mean()
-        self.extras['pose_diff_penalty'] = pose_diff_penalty.mean()
-        self.extras['work_done'] = work_penalty.mean()
-        self.extras['torques'] = torque_penalty.mean()
-        self.extras['roll'] = self.object_angvel[:, 0].mean()
-        self.extras['pitch'] = self.object_angvel[:, 1].mean()
-        self.extras['yaw'] = self.object_angvel[:, 2].mean()
-
-        if self.evaluate:
-            finished_episode_mask = self.reset_buf == 1
-            self.stat_sum_rewards += self.rew_buf.sum()
-            self.stat_sum_rotate_rewards += log_r_reward.sum()
-            self.stat_sum_torques += self.torques.abs().sum()
-            self.stat_sum_obj_linvel += (self.object_linvel ** 2).sum(-1).sum()
-            self.stat_sum_episode_length += (self.reset_buf == 0).sum()
-            self.env_evaluated += (self.reset_buf == 1).sum()
-            self.env_timeout_counter[finished_episode_mask] += 1
-            info = f'progress {self.env_evaluated} / {self.max_evaluate_envs} | ' \
-                   f'reward: {self.stat_sum_rewards / self.env_evaluated:.2f} | ' \
-                   f'eps length: {self.stat_sum_episode_length / self.env_evaluated:.2f} | ' \
-                   f'rotate reward: {self.stat_sum_rotate_rewards / self.env_evaluated:.2f} | ' \
-                   f'lin vel (x100): {self.stat_sum_obj_linvel * 100 / self.stat_sum_episode_length:.4f} | ' \
-                   f'command torque: {self.stat_sum_torques / self.stat_sum_episode_length:.2f}'
-            tprint(info)
-            if self.env_evaluated >= self.max_evaluate_envs:
-                exit()
-
-    def post_physics_step(self):
-        self.progress_buf += 1
-        self.reset_buf[:] = 0
-        self._refresh_gym()
-        self.compute_reward(self.actions)
-        env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)
-        if len(env_ids) > 0:
-            self.reset_idx(env_ids)
-        self.compute_observations()
-
-        if self.viewer and self.debug_viz:
-            # draw axes on target object
-            self.gym.clear_lines(self.viewer)
-            self.gym.refresh_rigid_body_state_tensor(self.sim)
-
-            for i in range(self.num_envs):
-                objectx = (self.object_pos[i] + quat_apply(self.object_rot[i], to_torch([1, 0, 0], device=self.device) * 0.2)).cpu().numpy()
-                objecty = (self.object_pos[i] + quat_apply(self.object_rot[i], to_torch([0, 1, 0], device=self.device) * 0.2)).cpu().numpy()
-                objectz = (self.object_pos[i] + quat_apply(self.object_rot[i], to_torch([0, 0, 1], device=self.device) * 0.2)).cpu().numpy()
-
-                p0 = self.object_pos[i].cpu().numpy()
-                self.gym.add_lines(self.viewer, self.envs[i], 1, [p0[0], p0[1], p0[2], objectx[0], objectx[1], objectx[2]], [0.85, 0.1, 0.1])
-                self.gym.add_lines(self.viewer, self.envs[i], 1, [p0[0], p0[1], p0[2], objecty[0], objecty[1], objecty[2]], [0.1, 0.85, 0.1])
-                self.gym.add_lines(self.viewer, self.envs[i], 1, [p0[0], p0[1], p0[2], objectz[0], objectz[1], objectz[2]], [0.1, 0.1, 0.85])
-
-    def _create_ground_plane(self):
-        plane_params = gymapi.PlaneParams()
-        plane_params.normal = gymapi.Vec3(0.0, 0.0, 1.0)
-        self.gym.add_ground(self.sim, plane_params)
-
-    def pre_physics_step(self, actions):
-        self.actions = actions.clone().to(self.device)
-        targets = self.prev_targets + 1 / 24 * self.actions
-        self.cur_targets[:] = tensor_clamp(targets, self.allegro_hand_dof_lower_limits, self.allegro_hand_dof_upper_limits)
-        self.prev_targets[:] = self.cur_targets.clone()
-
-        if self.force_scale > 0.0:
-            self.rb_forces *= torch.pow(self.force_decay, self.dt / self.force_decay_interval)
-            # apply new forces
-            obj_mass = to_torch(
-                [self.gym.get_actor_rigid_body_properties(env, self.gym.find_actor_handle(env, 'object'))[0].mass for
-                 env in self.envs], device=self.device)
-            prob = self.random_force_prob_scalar
-            force_indices = (torch.less(torch.rand(self.num_envs, device=self.device), prob)).nonzero()
-            self.rb_forces[force_indices, self.object_rb_handles, :] = torch.randn(
-                self.rb_forces[force_indices, self.object_rb_handles, :].shape,
-                device=self.device) * obj_mass[force_indices, None] * self.force_scale
-            self.gym.apply_rigid_body_force_tensors(self.sim, gymtorch.unwrap_tensor(self.rb_forces), None, gymapi.ENV_SPACE)
-
-    def reset(self):
-        super().reset()
-        self.obs_dict['priv_info'] = self.priv_info_buf.to(self.rl_device)
-        self.obs_dict['proprio_hist'] = self.proprio_hist_buf.to(self.rl_device)
-        return self.obs_dict
-
-    def step(self, actions):
-        super().step(actions)
-        self.obs_dict['priv_info'] = self.priv_info_buf.to(self.rl_device)
-        self.obs_dict['proprio_hist'] = self.proprio_hist_buf.to(self.rl_device)
-        return self.obs_dict, self.rew_buf, self.reset_buf, self.extras
-
-    def update_low_level_control(self):
-        previous_dof_pos = self.allegro_hand_dof_pos.clone()
-        self._refresh_gym()
-        if self.torque_control:
-            dof_pos = self.allegro_hand_dof_pos
-            dof_vel = (dof_pos - previous_dof_pos) / self.dt
-            self.dof_vel_finite_diff = dof_vel.clone()
-            torques = self.p_gain * (self.cur_targets - dof_pos) - self.d_gain * dof_vel
-            self.torques = torch.clip(torques, -0.5, 0.5).clone()
-            self.gym.set_dof_actuation_force_tensor(self.sim, gymtorch.unwrap_tensor(self.torques))
-        else:
-            self.gym.set_dof_position_target_tensor(self.sim, gymtorch.unwrap_tensor(self.cur_targets))
-
-    def check_termination(self, object_pos):
-        resets = torch.logical_or(
-            torch.less(object_pos[:, -1], self.reset_z_threshold),
-            torch.greater_equal(self.progress_buf, self.max_episode_length),
-        )
-        return resets
-
-    def _refresh_gym(self):
-        self.gym.refresh_dof_state_tensor(self.sim)
-        self.gym.refresh_actor_root_state_tensor(self.sim)
-        self.gym.refresh_rigid_body_state_tensor(self.sim)
-        self.gym.refresh_net_contact_force_tensor(self.sim)
-        self.object_pose = self.root_state_tensor[self.object_indices, 0:7]
-        self.object_pos = self.root_state_tensor[self.object_indices, 0:3]
-        self.object_rot = self.root_state_tensor[self.object_indices, 3:7]
-        self.object_linvel = self.root_state_tensor[self.object_indices, 7:10]
-        self.object_angvel = self.root_state_tensor[self.object_indices, 10:13]
-
-    def _setup_domain_rand_config(self, rand_config):
-        self.randomize_mass = rand_config['randomizeMass']
-        self.randomize_mass_lower = rand_config['randomizeMassLower']
-        self.randomize_mass_upper = rand_config['randomizeMassUpper']
-        self.randomize_com = rand_config['randomizeCOM']
-        self.randomize_com_lower = rand_config['randomizeCOMLower']
-        self.randomize_com_upper = rand_config['randomizeCOMUpper']
-        self.randomize_friction = rand_config['randomizeFriction']
-        self.randomize_friction_lower = rand_config['randomizeFrictionLower']
-        self.randomize_friction_upper = rand_config['randomizeFrictionUpper']
-        self.randomize_scale = rand_config['randomizeScale']
-        self.scale_list_init = rand_config['scaleListInit']
-        self.randomize_scale_list = rand_config['randomizeScaleList']
-        self.randomize_scale_lower = rand_config['randomizeScaleLower']
-        self.randomize_scale_upper = rand_config['randomizeScaleUpper']
-        self.randomize_pd_gains = rand_config['randomizePDGains']
-        self.randomize_p_gain_lower = rand_config['randomizePGainLower']
-        self.randomize_p_gain_upper = rand_config['randomizePGainUpper']
-        self.randomize_d_gain_lower = rand_config['randomizeDGainLower']
-        self.randomize_d_gain_upper = rand_config['randomizeDGainUpper']
-        self.joint_noise_scale = rand_config['jointNoiseScale']
-
-    def _setup_priv_option_config(self, p_config):
-        self.enable_priv_obj_position = p_config['enableObjPos']
-        self.enable_priv_obj_mass = p_config['enableObjMass']
-        self.enable_priv_obj_scale = p_config['enableObjScale']
-        self.enable_priv_obj_com = p_config['enableObjCOM']
-        self.enable_priv_obj_friction = p_config['enableObjFriction']
-
-    def _update_priv_buf(self, env_id, name, value, lower=None, upper=None):
-        # normalize to -1, 1
-        s, e = self.priv_info_dict[name]
-        if eval(f'self.enable_priv_{name}'):
-            if type(value) is list:
-                value = to_torch(value, dtype=torch.float, device=self.device)
-            if type(lower) is list or upper is list:
-                lower = to_torch(lower, dtype=torch.float, device=self.device)
-                upper = to_torch(upper, dtype=torch.float, device=self.device)
-            if lower is not None and upper is not None:
-                value = (2.0 * value - upper - lower) / (upper - lower)
-            self.priv_info_buf[env_id, s:e] = value
-        else:
-            self.priv_info_buf[env_id, s:e] = 0
-
-    def _setup_object_info(self, o_config):
-        self.object_type = o_config['type']
-        raw_prob = o_config['sampleProb']
-        assert (sum(raw_prob) == 1)
-
-        primitive_list = self.object_type.split('+')
-        print('---- Primitive List ----')
-        print(primitive_list)
-        self.object_type_prob = []
-        self.object_type_list = []
-        self.asset_files_dict = {
-            'simple_tennis_ball': 'assets/ball.urdf',
-        }
-        for p_id, prim in enumerate(primitive_list):
-            if 'cuboid' in prim:
-                subset_name = self.object_type.split('_')[-1]
-                cuboids = sorted(glob(f'../assets/cuboid/{subset_name}/*.urdf'))
-                cuboid_list = [f'cuboid_{i}' for i in range(len(cuboids))]
-                self.object_type_list += cuboid_list
-                for i, name in enumerate(cuboids):
-                    self.asset_files_dict[f'cuboid_{i}'] = name.replace('../assets/', '')
-                self.object_type_prob += [raw_prob[p_id] / len(cuboid_list) for _ in cuboid_list]
-            elif 'cylinder' in prim:
-                subset_name = self.object_type.split('_')[-1]
-                cylinders = sorted(glob(f'assets/cylinder/{subset_name}/*.urdf'))
-                cylinder_list = [f'cylinder_{i}' for i in range(len(cylinders))]
-                self.object_type_list += cylinder_list
-                for i, name in enumerate(cylinders):
-                    self.asset_files_dict[f'cylinder_{i}'] = name.replace('../assets/', '')
-                self.object_type_prob += [raw_prob[p_id] / len(cylinder_list) for _ in cylinder_list]
-            else:
-                self.object_type_list += [prim]
-                self.object_type_prob += [raw_prob[p_id]]
-        print('---- Object List ----')
-        print(self.object_type_list)
-        assert (len(self.object_type_list) == len(self.object_type_prob))
-
-    def _allocate_task_buffer(self, num_envs):
-        # extra buffers for observe randomized params
-        self.prop_hist_len = self.config['env']['hora']['propHistoryLen']
-        self.num_env_factors = self.config['env']['hora']['privInfoDim']
-        self.priv_info_buf = torch.zeros((num_envs, self.num_env_factors), device=self.device, dtype=torch.float)
-        self.proprio_hist_buf = torch.zeros((num_envs, self.prop_hist_len, 32), device=self.device, dtype=torch.float)
-
-    def _setup_reward_config(self, r_config):
-        self.angvel_clip_min = r_config['angvelClipMin']
-        self.angvel_clip_max = r_config['angvelClipMax']
-        self.rotate_reward_scale = r_config['rotateRewardScale']
-        self.object_linvel_penalty_scale = r_config['objLinvelPenaltyScale']
-        self.pose_diff_penalty_scale = r_config['poseDiffPenaltyScale']
-        self.torque_penalty_scale = r_config['torquePenaltyScale']
-        self.work_penalty_scale = r_config['workPenaltyScale']
-
-    def _create_object_asset(self):
-        # object file to asset
-        asset_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
-        hand_asset_file = self.config['env']['asset']['handAsset']
-        # load hand asset
-        hand_asset_options = gymapi.AssetOptions()
-        hand_asset_options.flip_visual_attachments = False
-        hand_asset_options.fix_base_link = True
-        hand_asset_options.collapse_fixed_joints = True
-        hand_asset_options.disable_gravity = True
-        hand_asset_options.thickness = 0.001
-        hand_asset_options.angular_damping = 0.01
-
-        if self.torque_control:
-            hand_asset_options.default_dof_drive_mode = gymapi.DOF_MODE_EFFORT
-        else:
-            hand_asset_options.default_dof_drive_mode = gymapi.DOF_MODE_POS
-        self.hand_asset = self.gym.load_asset(self.sim, asset_root, hand_asset_file, hand_asset_options)
-
-        # load object asset
-        self.object_asset_list = []
-        for object_type in self.object_type_list:
-            object_asset_file = self.asset_files_dict[object_type]
-            object_asset_options = gymapi.AssetOptions()
-            object_asset = self.gym.load_asset(self.sim, asset_root, object_asset_file, object_asset_options)
-            self.object_asset_list.append(object_asset)
-
-    def _init_object_pose(self):
-        allegro_hand_start_pose = gymapi.Transform()
-        allegro_hand_start_pose.p = gymapi.Vec3(0, 0, 0.5)
-        allegro_hand_start_pose.r = gymapi.Quat.from_axis_angle(
-            gymapi.Vec3(0, 1, 0), -np.pi / 2) * gymapi.Quat.from_axis_angle(gymapi.Vec3(1, 0, 0), np.pi / 2)
-        object_start_pose = gymapi.Transform()
-        object_start_pose.p = gymapi.Vec3()
-        object_start_pose.p.x = allegro_hand_start_pose.p.x
-        pose_dx, pose_dy, pose_dz = -0.01, -0.04, 0.15
-
-        object_start_pose.p.x = allegro_hand_start_pose.p.x + pose_dx
-        object_start_pose.p.y = allegro_hand_start_pose.p.y + pose_dy
-        object_start_pose.p.z = allegro_hand_start_pose.p.z + pose_dz
-
-        object_start_pose.p.y = allegro_hand_start_pose.p.y - 0.01
-        # for grasp pose generation, it is used to initialize the object
-        # it should be slightly higher than the fingertip
-        # so it is set to be 0.66 for internal allegro and 0.64 for the public allegro
-        # ----
-        # for in-hand object rotation, the initialization of z is only used in the first step
-        # it is set to be 0.65 for backward compatibility
-        object_z = 0.66 if self.save_init_pose else 0.65
-        if 'internal' not in self.grasp_cache_name:
-            object_z -= 0.02
-        object_start_pose.p.z = object_z
-        return allegro_hand_start_pose, object_start_pose
-
-
-def compute_hand_reward(
-    object_linvel, object_linvel_penalty_scale: float,
-    object_angvel, rotation_axis, rotate_reward_scale: float,
-    angvel_clip_max: float, angvel_clip_min: float,
-    pose_diff_penalty, pose_diff_penalty_scale: float,
-    torque_penalty, torque_pscale: float,
-    work_penalty, work_pscale: float,
-):
-    rotate_reward_cond = (rotation_axis[:, -1] != 0).float()
-    vec_dot = (object_angvel * rotation_axis).sum(-1)
-    rotate_reward = torch.clip(vec_dot, max=angvel_clip_max, min=angvel_clip_min)
-    rotate_reward = rotate_reward_scale * rotate_reward * rotate_reward_cond
-    object_linvel_penalty = torch.norm(object_linvel, p=1, dim=-1)
-
-    reward = rotate_reward
-    # Distance from the hand to the object
-    reward = reward + object_linvel_penalty * object_linvel_penalty_scale
-    reward = reward + pose_diff_penalty * pose_diff_penalty_scale
-    reward = reward + torque_penalty * torque_pscale
-    reward = reward + work_penalty * work_pscale
-    return reward, rotate_reward, object_linvel_penalty
diff --git a/manipulation_gym/tasks/base/vec_task.py b/manipulation_gym/tasks/base/vec_task.py
index 5a11b3a..925a4a3 100755
--- a/manipulation_gym/tasks/base/vec_task.py
+++ b/manipulation_gym/tasks/base/vec_task.py
@@ -135,6 +135,8 @@ class VecTask(Env):
         """
         super().__init__(config, sim_device, graphics_device_id, headless)
 
+        
+
         self.sim_params = self._parse_sim_params(config['physics_engine'], config['sim'])
         if config['physics_engine'] == 'physx':
             self.physics_engine = gymapi.SIM_PHYSX
@@ -150,6 +152,9 @@ class VecTask(Env):
 
         self.gym = gymapi.acquire_gym()
         self._allocate_buffers()
+
+        
+
         # create envs, sim and viewer
         self.create_sim()
         self.gym.prepare_sim(self.sim)
diff --git a/manipulation_gym/tasks/open_manipulator_x.py b/manipulation_gym/tasks/open_manipulator_x.py
deleted file mode 100644
index e69de29..0000000
diff --git a/manipulation_gym/utils/__pycache__/__init__.cpython-38.pyc b/manipulation_gym/utils/__pycache__/__init__.cpython-38.pyc
old mode 100755
new mode 100644
index 4e1f06d..8ee696d
Binary files a/manipulation_gym/utils/__pycache__/__init__.cpython-38.pyc and b/manipulation_gym/utils/__pycache__/__init__.cpython-38.pyc differ
diff --git a/manipulation_gym/utils/__pycache__/misc.cpython-38.pyc b/manipulation_gym/utils/__pycache__/misc.cpython-38.pyc
old mode 100755
new mode 100644
index f9a1599..27815a3
Binary files a/manipulation_gym/utils/__pycache__/misc.cpython-38.pyc and b/manipulation_gym/utils/__pycache__/misc.cpython-38.pyc differ
diff --git a/manipulation_gym/utils/__pycache__/reformat.cpython-38.pyc b/manipulation_gym/utils/__pycache__/reformat.cpython-38.pyc
old mode 100755
new mode 100644
index 6b255e6..b9b6c64
Binary files a/manipulation_gym/utils/__pycache__/reformat.cpython-38.pyc and b/manipulation_gym/utils/__pycache__/reformat.cpython-38.pyc differ
diff --git a/scripts/train_s1.sh b/scripts/train_s1.sh
index 6d9a113..690b8f7 100755
--- a/scripts/train_s1.sh
+++ b/scripts/train_s1.sh
@@ -11,10 +11,10 @@ EXTRA_ARGS_SLUG=${EXTRA_ARGS// /_}
 echo extra "${EXTRA_ARGS}"
 
 CUDA_VISIBLE_DEVICES=${GPUS} \
-python train.py task=AllegroHandHora headless=True seed=${SEED} \
+python train.py task=OpenManipulatorPick headless=False seed=${SEED} \
 task.env.forceScale=2 task.env.randomForceProbScalar=0.25 \
 train.algo=PPO \
-task.env.object.type=cylinder_default \
+task.env.object.type=cuboid_default \
 train.ppo.priv_info=True train.ppo.proprio_adapt=False \
-train.ppo.output_name=AllegroHandHora/"${CACHE}" \
+train.ppo.output_name=OpenManipulationPick/"${CACHE}" \
 ${EXTRA_ARGS}
\ No newline at end of file
diff --git a/train.py b/train.py
index 55be2f0..1a85fc0 100755
--- a/train.py
+++ b/train.py
@@ -27,11 +27,11 @@ from termcolor import cprint
 from omegaconf import DictConfig, OmegaConf
 from hydra.utils import to_absolute_path
 
-from hora.algo.ppo.ppo import PPO
-from hora.algo.padapt.padapt import ProprioAdapt
-from hora.tasks import isaacgym_task_map
-from hora.utils.reformat import omegaconf_to_dict, print_dict
-from hora.utils.misc import set_np_formatting, set_seed, git_hash, git_diff_config
+from manipulation_gym.algo.ppo.ppo import PPO
+from manipulation_gym.algo.padapt.padapt import ProprioAdapt
+from manipulation_gym.tasks import isaacgym_task_map
+from manipulation_gym.utils.reformat import omegaconf_to_dict, print_dict
+from manipulation_gym.utils.misc import set_np_formatting, set_seed, git_hash, git_diff_config
 
 
 ## OmegaConf & Hydra Config
@@ -64,6 +64,8 @@ def main(config: DictConfig):
         headless=config.headless,
     )
 
+    
+
     output_dif = os.path.join('outputs', config.train.ppo.output_name)
     os.makedirs(output_dif, exist_ok=True)
     agent = eval(config.train.algo)(env, output_dif, full_config=config)
@@ -73,7 +75,7 @@ def main(config: DictConfig):
         agent.test()
     else:
         date = str(datetime.datetime.now().strftime('%m%d%H'))
-        print(git_diff_config('./'))
+        # print(git_diff_config('./'))
         os.system(f'git diff HEAD > {output_dif}/gitdiff.patch')
         with open(os.path.join(output_dif, f'config_{date}_{git_hash()}.yaml'), 'w') as f:
             f.write(OmegaConf.to_yaml(config))
@@ -89,7 +91,10 @@ def main(config: DictConfig):
             if user_input != 'yes':
                 exit()
 
+        print(f"Load train checkpoint: {config.train.load_path}")
         agent.restore_train(config.train.load_path)
+
+        print("Starting training")
         agent.train()
 
 
